{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación e importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg\n"
     ]
    }
   ],
   "source": [
    "print(\"fg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow==2.1.0\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_entreno = 'C:/Users/PAUL/Desktop/jupyter/data/'\n",
    "dir_entreno_mas = os.path.join(dir_entreno, 'with_mask')\n",
    "dir_entreno_no_mas = os.path.join(dir_entreno, 'without_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-with-mask.jpg', '1-with-mask.jpg', '10-with-mask.jpg', '100-with-mask.jpg', '101-with-mask.jpg', '103-with-mask.jpg', '104-with-mask.jpg', '105-with-mask.jpg', '106-with-mask.jpg', '107-with-mask.jpg']\n",
      "['0.jpg', '1.jpg', '10.jpg', '100.jpg', '101.jpg', '102.jpg', '104.jpg', '105.jpg', '106.jpg', '107.jpg']\n"
     ]
    }
   ],
   "source": [
    "entreno_masc_fnames = os.listdir( dir_entreno_mas )\n",
    "entreno_no_masc_fnames = os.listdir( dir_entreno_no_mas )\n",
    "print(entreno_masc_fnames[:10])\n",
    "print(entreno_no_masc_fnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imagenes con mascara:  700\n",
      "Total imagenes sin mascara:  696\n"
     ]
    }
   ],
   "source": [
    "print('Total imagenes con mascara: ', len(os.listdir( dir_entreno_mas ) ))\n",
    "print('Total imagenes sin mascara: ', len(os.listdir( dir_entreno_no_mas ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un modelo con 3 capas de convolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    #clasifica si hay o no\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1117 images belonging to 2 classes.\n",
      "Found 279 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "batch_size = 10\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "            dir_entreno,\n",
    "            target_size=(img_height, img_width),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='binary',\n",
    "            subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "            dir_entreno,\n",
    "            target_size=(img_height, img_width),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='binary',\n",
    "            subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 111 steps, validate for 27 steps\n",
      "Epoch 1/10\n",
      "111/111 [==============================] - 108s 969ms/step - loss: 0.3106 - accuracy: 0.8699 - val_loss: 0.1758 - val_accuracy: 0.9481\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 55s 495ms/step - loss: 0.1433 - accuracy: 0.9539 - val_loss: 0.1038 - val_accuracy: 0.9593\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 61s 545ms/step - loss: 0.0728 - accuracy: 0.9774 - val_loss: 0.1011 - val_accuracy: 0.9630\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 60s 543ms/step - loss: 0.0997 - accuracy: 0.9720 - val_loss: 0.1865 - val_accuracy: 0.9370\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - 52s 465ms/step - loss: 0.0648 - accuracy: 0.9783 - val_loss: 0.0553 - val_accuracy: 0.9815\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 51s 455ms/step - loss: 0.0528 - accuracy: 0.9810 - val_loss: 0.0391 - val_accuracy: 0.9852\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 55s 493ms/step - loss: 0.0191 - accuracy: 0.9955 - val_loss: 0.0692 - val_accuracy: 0.9815\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 58s 520ms/step - loss: 0.0240 - accuracy: 0.9910 - val_loss: 0.0507 - val_accuracy: 0.9852\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - 56s 505ms/step - loss: 0.0448 - accuracy: 0.9837 - val_loss: 0.0464 - val_accuracy: 0.9852\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 54s 486ms/step - loss: 0.0235 - accuracy: 0.9910 - val_loss: 0.0585 - val_accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 10\n",
    "history = model.fit_generator(\n",
    "                    train_generator,\n",
    "                    steps_per_epoch = train_generator.samples // batch_size,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = validation_generator.samples // batch_size,\n",
    "                    epochs = nb_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = cv2.VideoCapture(0)\n",
    "plt.ion()\n",
    "if vc.isOpened(): \n",
    "    is_capturing, frame = vc.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    \n",
    "    webcam_preview = plt.imshow(frame)    \n",
    "else:\n",
    "    is_capturing = False\n",
    "\n",
    "while is_capturing:\n",
    "    try:    \n",
    "        is_capturing, frame = vc.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    \n",
    "        frame_res = cv2.resize(frame, dsize=(150, 150), interpolation=cv2.INTER_CUBIC)\n",
    "        x=image.img_to_array(frame_res)\n",
    "        x=np.expand_dims(x, axis=0)\n",
    "        images = np.vstack([x])\n",
    "        classes = model.predict(images, batch_size=10)\n",
    "        if classes[0]>0:\n",
    "            print(\"No Lleva mascara\")\n",
    "        else:\n",
    "            print(\"Sí lleva mascara\")\n",
    "        webcam_preview = plt.imshow(frame)\n",
    "        try:    \n",
    "            plt.pause(1)\n",
    "        except Exception:\n",
    "            pass\n",
    "    except KeyboardInterrupt:\n",
    "        vc.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convertir el modelo a una versión liguera TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37981232"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"mask_classifier.tflite\",\"wb\").write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
